% -*- mode: LaTeX; coding: utf-8; -*-

\chapter{Tiedon keruu ja käsittely}

Tiedon käsittely jaetaan esikäsittelyyn, menetelmään ja
jälkikäsittelyyn. Esikäsittelyn aikana palvelinlokit muutetaan sellaiseen
muotoon, jota menetelmä pystyy käsittelemään. Menetelmävaiheessa
aineisto ajetaan diffuusiokuvausalgoritmin läpi. Jälkikäsittelyssä sitten
luetaan diffuusiokuvauksen tuottamaa dataa ja selvitetään
mielenkiintoisten havaintojen taustalla olevat HTTP-kyselyt.

Tässä luvussa tutustutaan aluksi tutkittavien lokitiedostojen
alkuperään ja tiedostomuotoon sekä aineiston arkistointitapaan. Tämän
jälkeen käydään läpi, kuinka tietoa on esikäsitelty analyysiä silmällä
pitäen. Käytetyt menetelmät soveltuvat pienin muutoksin myös muiden
Web-palveluntarjoajien palvelinlokien esikäsittelyyn, mikäli
arkistointikäytänteet eivät suuresti poikkea tässä esitellystä.

\section{Aineiston rakenne}
\label{sec:lahtokohta}

Analysoitavaksi saamamme loki on peräisin yritykseltä, joka toimii
Web-\-palveluntarjoajana suuryrityksille. Lokitiedostot ovat peräisin
kolmelta tuotannosta jo poistetuilta palvelimilta, ja ne ovat olleet
vastuussa muutaman suuren palvelukokonaisuuden pyörittämisestä.

Analysoitava data on Apache-palvelimien tuottamaa lokia, joka sisältää
Web-palveluille kohdistuvia HTTP-pyyntöjä. Lokia on yhteensä 24
gigatavua, ja se sisältää yli 1,7 miljardia sivupyyntöä.
Lokia on kerätty noin 10 kuukauden
ajanjaksolta. Se on tallennettu \textit{Combined Log Format} -muotoon,
joka on yleinen Apache-palvelimen käyttämä lokitiedoston
muoto~\cite{combined}. Listauksessa \ref{httpkys} on esimerkki onnistuneen HTTP-kyselyn seurauksena muodostuneesta lokirivistä.

% Tarkat luvut:
% - Dataa 24133108 kiB
% - Sivupyyntöjä (ml. error-logit) on 1666171347 kpl.

\begin{lstlisting}[float=h,caption=Esimerkki onnistuneesta HTTP-kyselystä.,label=httpkys,aboveskip=0.5cm]
130.234.49.2 - - [10/May/2009:15:53:01 +0300]
"GET /scripts/access.pl?user=matti&passwd=admin HTTP/1.1"
200 2680 "http://www.jyu.fi/a.html"
"Mozilla/5.0 (SymbianOS/9.2;...)"
\end{lstlisting}

Web-palvelimien tuottama loki sisältää paljon tietoa muun muassa
palveluiden käyttöasteista, ja niihin kohdistuvista kuormista. Lokeja
analysoimalla voidaan myös tunnistaa mahdollisia hyökkäysyrityksiä,
sekä hyökkäyksen jo tapahduttua tutkia siihen johtaneita
vaiheita. Pienillä sivustoilla ihmisen on mahdollista läpikäydä
lokitiedostot hyökkäyksien varalta, mutta puhuttaessa palveluista,
joilla on miljoonia käyttäjiä kuukaudessa, ei käsin läpikäyminen ole
enää käytännössä mahdollista. Tästä syystä lokien
analysoiminen tulee automatisoida. Analysoinnille on myös kovat
laatuvaatimukset, koska järjestelmän tulisi pystyä tunnistamaan
miljoonien kyselyiden joukosta ne, jotka ovat syntyneet hyökkäyksen
johdosta.

Yksi mahdollisuus on käyttää sääntöpohjaisia suodattimia, mutta
aikaisemmissa luvuissa esitetyistä syistä johtuen ei tämä tarjoa aina riittävää
tunnistuskykyä. Siksi tässä tutkimuksessa on päädytty käyttämään
diffuusiokuvauksiin perustuvaa anomalioiden tunnistamismenetelmää. 
Tässä järjestelmälle opetetaan ensiksi normaali käyttäytyminen, jonka jälkeen analysoitavaa
lokia verrataan opetettuun malliin. Näin poikkeava liikenne voidaan tunnistaa.

Tietoturvan kannalta kiinnostavin osa lokista on HTTP-kyselyn
osoiteosa, jossa viitataan johonkin resurssiin ja välitetään
tarvittaessa parametreja. HTTP-kysely sisältää
niin staattiset sivunlatauspyynnöt kuin myös palvelimelle
välitettävät parametriarvot. Tällaisia voivat olla esimerkiksi
kirjautumisessa käytetyt tiedot ja tietokannalle välitetyt
kyselyt.

Kysely koostuu metodista, resurssista ja parametriosasta.  Metodin
jälkeen välilyönnillä erotettuna seuraa osoiteosa. Se jakautuu
resurssiin ja parametreihin, jossa erottimena toimii kysymysmerkki.
Parametriosa koostuu parametrilistasta, jossa parametrit ovat erotettu
toisistaan \texttt{\&}-merkillä. Yksittäinen parametri on
avain-arvopari, jossa avain on yhtäsuurusmerkin vasemmalla puolella ja
arvo oikealla puolella (kuva \ref{CLF2}). Käytettävien parametrien
lukumäärä vaihtelee palvelusta riippuen. Mikäli parametreja ei ole
lainkaan, ei parametriosaa eikä sitä edeltävää kysymysmerkkiä esiinny
kyselyssä. Näin on esimerkiksi silloin, kun HTTP-kysely kohdistuu
staattiseen Web-sivuun.

\vskip 0.5cm
\begin{figure}[ht]
\[
\overbrace{\texttt{GET}}^\text{metodi}
\overbrace{\texttt{/scripts/access.pl}}^\text{resurssi}
\texttt{?}
\overbrace{\underbrace{\texttt{user}}_\text{avain 1}
\texttt{=}
\underbrace{\texttt{matti}}_\text{arvo 1}
\texttt{\&}
\underbrace{\texttt{passwd}}_\text{avain 2}
\texttt{=}
\underbrace{\texttt{admin}}_\text{arvo 2}}
^{\text{parametrit}}
\]
\caption{HTTP GET -kyselyn rakenne.}
\label{CLF2}
\end{figure}


\section{Arkistointikäytänteet}

Web-hotelli, josta data on peräisin, on toteutettu siten, että yksittäiset palvelut on
sijoitettu useammalle palvelimelle. Ratkaisun taustalla on
kuormituksen tasaaminen. Erillinen järjestelmä huolehtii siitä, että
Internetistä tulevat kyselyt ohjataan tasaisesti eri
palvelimille. Tästä johtuen samaan palveluun kohdistuvat kyselyt
jakautuvat useamman palvelimen lokitiedostoihin.

Taulukossa \ref{nimet} on esimerkki palveluiden ja palvelinten
nimeämisestä. Esimerkeissä käytetyt palvelinten ja palveluiden nimet
ovat kuvitteellisia, mutta rakenne vastaa tutkittua ympäristöä.

% Ei kannata ottaa esimerkkiä tästä taulukosta, tämä on vähän mutkikas.
\begin{table}[h]
\centering
\begin{tabular}{lll}
Palvelin && Palvelu \\
\cline{1-1}\cline{3-3}
dapper && buzz \\
edgy && rex \\
feisty && potato \\
&& hamm \\
\end{tabular}
\caption{Palvelinten ja palveluiden nimeäminen.}
\label{nimet}
\end{table}

Lokitiedostot on sijoitettu hakemistorakenteeseen, jossa juuressa ovat
palvelinten nimien mukaiset hakemistot, joiden sisällä sijaitsevat
lokitiedostot, jotka on nimetään yhdistämällä palvelun nimi
päivämääräleimaan. Tiedostonnimessä käytetty päivämäärän muoto
noudattaa ISO 8601 -standardia~\cite{iso8601}, ja tiedostot on pakattu
\texttt{gzip}-pakkausohjelmalla. Taulukossa \ref{tiedostot}
on havainnollistettu tiedostonnimien muodostuminen.

\begin{table}[h]
\centering
\begin{tabular}{llll}
Palvelin & Palvelu & Päivämäärä & Tiedostonnimi \\
\hline
edgy & buzz & 14.6.2009 & \texttt{edgy/buzz.http.2009-06-14.gz}\\ 
dapper & potato & 2.7.2009 & \texttt{dapper/potato.http.2009-07-02.gz}\\
feisty & rex & 30.7.2009 & \texttt{feisty/rex.http.2009-07-30.gz}\\
\end{tabular}
\caption{Tiedostojen nimeäminen.}
\label{tiedostot}
\end{table}

Tässä mainittujen palvelinten lisäksi käytössä on ulkoinen
välimuistipalvelu, jonka kautta välitetään harvoin muuttuvia
resursseja, kuten kuvia. Valtaosa dataliikenteestä
välitetään palvelun kautta. Välimuistipalvelu toimii siten, että
mikäli pyydettyä resurssia ei löydy sen omasta muistista, se pyytää
sitä yhdeltä Web-hotellin palvelimista ja välittää vastauksen edelleen
asiakkaalle. Välimuistipalvelun käyttö ei kuitenkaan muilta osin
vaikuta palvelun rakenteeseen. Tämä rakenne käy ilmi kuvasta
\ref{palvelinrakenne}.

\begin{figure}[htp]
\centering
\includegraphics[width=12cm]{pics/palvelinrakenne.pdf}
\caption{Web-palvelun rakenne.}
\label{palvelinrakenne}
\end{figure}

\section{Esikäsittely}

Luvussa \ref{sec:lahtokohta} esiteltiin lokitiedostoissa käytetty
\textit{Combined Log Format} -muoto. Tiedosto on tekstimuotoinen,
joten se ei sellaisenaan sovellu analyysivaiheeseen, jossa käsitellään
luokka-asteikollisia muuttujia. Osa palvelinlokin sisällöstä on myös analyysin kannalta
tarpeetonta ja nämä osat tulee suodattaa pois. Lisäksi yhteen
Web-palveluun liittyvät kyselyt ovat jakautuneet useaan eri
tiedostoon eri palvelinten ja vuorokausien mukaisesti.

Tässä työssä käytetty esikäsittelijä on toteutettu osana tätä tutkimusta. Sovelluksen 
nimi on \textit{PhasefulSplitter}, ja se on ohjelmoitu käyttäen
Haskell-ohjelmointikielellä \cite{haskell98}. Haskell on valittu, koska se tarjoaa
tehokkaat työkalut ohjelmakoodin rinnakkaistamiseen ja datan
sarjallistamiseen. \textit{PhasefulSplitter} on vapaa ohjelma ja sitä saa
levittää edelleen ja muuttaa Free Software Foundationin julkaiseman
GNU General Public Licensen (GPL-lisenssi) version 3~\cite{gplv3} tai (valinnan
mukaan) myöhemmän version ehtojen mukaisesti. Sovellus on
ladattavissa Internetistä osoitteesta \\
\url{http://iki.fi/zouppen/repo/phasefulsplitter.git}.

Sopivaa yksivaiheista parseria käyttämällä olisi mahdollista käsitellä
lähtödata suoraan analyysissä käytettävään muotoon. Käytännössä
kuitenkin datan esikäsittely kannattaa hoitaa useammassa vaiheessa,
jotta datassa olevat puuttuvat tai poikkeavat arvot voidaan huomioida
ja esikäsittelijää voidaan korjata suorittamatta koko ajoa uudelleen
alusta lähtien. Monivaiheinen esikäsittely helpottaa myös datan
käsittelyä jälkikäteen erilaisin menetelmin. Lisäksi sarjallistetut
tietorakenteet voidaan tarvittaessa anonymisoida, jolloin dataa
voidaan luovuttaa myös ulkopuoliseen käyttöön.

Tiedon käsittelyn helpottamiseksi tässä työssä käytetään esikäsittelyn
välivaiheet ja lopputulokset tallennetaan väliaikaistiedostoihin.
Relaatiotietokantaa ei käytetä, koska tietokantakyselyiden
rinnakkaistaminen osoittautui hyvin vaikeaksi verrattuna suoraan
tiedostoja käsittelevään toteutukseen.

Esikäsittely jakaantuu seuraavaat neljään vaiheeseen:

\begin{enumerate}
\item Tiedostolistan muodostaminen ja tiedostojen ryhmittely palveluittain.
\item Tiedostojen sisällön lukeminen ja muuntaminen tietorakenteeksi.
\item N-grammien muodostaminen HTTP-kyselyn parametreista.
\item Aineiston muuntaminen matriisimuotoon analyysiä varten.
\end{enumerate}

Seuraavaksi käydään läpi esimerkkien tuella se, kuinka nämä vaiheet
suoritetaan.

\subsection{Tiedostolistan muodostaminen}
\label{sec:tiedostolista}

Tekstipohjainen tiedostolista luetaan
\textit{PhasefulSplitter}-ohjelman ymmärtämään muotoon, jolloin
tiedostonnimet ryhmitellään palveluiden nimien perusteella. 
Luokittelun yhteydessä myös palvelinten nimet korvataan numeerisilla
viitteillä. Tätä varten asetetaan tiedostoon \texttt{server.map}
palvelinten nimet ja niitä vastaavat numeeriset viitteet. Viitteitä
voidaan hyödyntää myöhemmin, jos halutaan muuntaa analyysivaiheessa
kiinnostava havainto takaisin Apache-lokin riviksi. Tässä luvun
esimerkissä käytettävän tiedoston sisältö on esitelty listauksessa
\ref{servermap}.

\begin{lstlisting}[language=MyHaskell,float=h,caption=Tiedoston server.map sisältö.,label=servermap]
[
      ("dapper",1)
    , ("edgy",2)
    , ("feisty",3)
]
\end{lstlisting}

Tiedostolista voidaan muodostaa
Linux-järjestelmässä listauksen \ref{filelist} mukaisesti.
Ajon yhteydessä muodostuu hakemiston \texttt{lists} alle palveluiden
mukaan nimetyt tiedostot. Tiedoston sisältönä on palveluun kuuluvat
tiedostonnimet sekä tiedon siitä, minkä palvelimen lokitiedostosta on kyse. Tiedostot ovat tekstimuotoon 
sarjallistettuja.

\begin{lstlisting}[language=bashshell,label=filelist,aboveskip=1cm,caption=Tiedostolistan
  muodostaminen.]
mkdir lists
find [[polku]] -iname '*.gz' | classifier lists/
\end{lstlisting}

\subsection{Muuntaminen tietorakenteeksi}

Toisessa vaiheessa tekstimuotoiset lokitiedostot luetaan ja
käsitellään koneellisesti helpommin analysoitavaan muotoon. Tätä työtä
varten kehitetyssä tiedonkäsittelijässä lokitiedoston rivin eri kentät
palastellaan ja kyselyt sarjallistetaan tiedostoihin. Tietorakenne on
esitelty listauksessa \ref{entry}.

\lstset{language=MyHaskell}

Koska jatkokäsittely voidaan hoitaa säikeistettynä useammalle
prosessorille tai ytimelle, tässä vaiheessa sovellukselle tulee
ilmoittaa säikeiden määrä. Tiedon perusteella sovellus jakaa yhteen
palveluun liittyvän datan haluttuun määrään erillisiä tiedostoja. Jako
mahdollistaa tehokkaan jälkikäsittelyn, koska tiedostojen sisältöä on
mahdollista käsitellä myöhemmin samanaikaisesti.

Linuxin \texttt{xargs}-komennolla voidaan myös tämän vaihe suorittaa
rinnakkaistettuna, jolloin eri palveluihin kuuluva data voidaan
muuntaa samanaikaisesti. Mikäli eri palveluihin kuuluvien lokien
datamäärä vaihtelee, ei rinnakkaistamisesta kuitenkaan saavuteta
täyttä hyötyä.  Listauksessa \ref{a2d} muunnetaan palvelinlokit
sovelluksen käyttämään muotoon. Käsiteltävät tiedostot luetaan
\texttt{lists}-hakemistosta löytyvien
tiedostolistauksista. Tietorakenteet kirjoitetaan hakemistoon
\texttt{data}. Listauksessa mainittu N korvataan tietokoneen
prosessorien tai ytimien määrällä.

\begin{lstlisting}[float=h,caption=Yhden lokirivin säilövä tietorakenne.,label=entry,aboveskip=1cm]
data Entry = Entry {
      info      :: LineInfo
    , ip        :: ByteString
    , date      :: UTCTime
    , method    :: ByteString
    , url       :: URL
    , protocol  :: ByteString
    , response  :: Integer
    , bytes     :: Integer
    , referer   :: ByteString
    , browser   :: ByteString
} deriving (Show,Eq)
\end{lstlisting}

\begin{lstlisting}[float=h,language=bashshell,label=a2d,caption=Muuntaminen
  tietorakenteeksi.]
ls lists/*| xargs -n 1 -I{} -P [[N]] apache2data [[N]] {} data/
\end{lstlisting}

Suoritusaika riippuu luonnollisesti koneen suorituskyvystä ja
lokitiedostojen määrästä. Tämän tutkimuksen yhteydessä suoritetussa 24
gigatavun ajossa 2,5 gigahertsin Intel Xeon -prosessorilla varustetussa
tietokoneessa tämän vaiheen suoritus kesti useita
prosessorivuorokausia.

\subsection{Parametrien N-grammianalyysi}

Palvelinlokissa olevista kyselyistä muodostetaan tietorakenne, jossa
yksi osa on kyselyn URL. Tässä vaiheessa tutkitaan URL-osoitetta
tarkemmin. Mikäli URL:n osana on parametreja, muodostetaan jokaisesta
parametrin arvosta 2-grammilistaus. Näin saadut 2-grammikartat
muodostetaan ja lopuksi ne tallennetaan matriisina tekstimuotoon
jatkokäsittelyä varten. Tiedostossa \texttt{ParameterAnalyzer.hs} on
määritelty vakiona, että lasketaan nimenomaisesti 2-grammit. Tätä
vakiota voi kuitenkin tarvittaessa muuttaa.

Ensimmäisessä ajossa selvitetään, mitkä mahdollisista 2-grammeista
ylipäätään esiintyvät aineistossa. Mahdollisia N-grammeja on yhteensä
$2^{8n}$ kappaletta, koska käsiteltävät merkkijonot koostuvat
Word8-tyypeistä (8-bittinen tavu). Säästääkseme muistia
analyysivaiheessa, selvitetään aluksi, millaisia 2-grammeja
aineistossa esiintyy ja jätetään taulukoimatta sellaiset 2-grammit,
jotka eivät esiinny lainkaan.

Jokaiselle resurssille muodostetaan oma N-grammilistansa. Datan määrä
luonnollisesti riippuu käytettävästä aineistosta. Käyttämällämme
datalla taulukon kooksi muodostui muutamia kymmeniä kilotavuja.
Lopputulos on siis vain murto-osa alkuperäisen datamassan
suuruudesta.

N-grammianalyysin ensimmäinen vaihe suoritetaan halutun
palvelun hakemistossa listauksen \ref{paraanal} mukaisesti.
Hakemistoon muodostuu ajon seurauksena tiedostot \texttt{ngrams.out}
ja \texttt{grams\_raw.txt}. Näistä ensimmäinen on binaarimuodossa ja
se sisältää toisessa vaiheessa tarvittavan datan. 
Toisessa tiedostossa on tekstimuodossa (Haskellin
Show-muodossa) eri resurssien ja 2-grammien
esiintymistiheydet. Tekstimuotoista tiedostoa voidaan käyttää apuna
arvioitaessa, mihin resursseihin ja parametreihin kannattaa kiinnittää
jatkossa huomiota.

\begin{lstlisting}[language=bashshell,aboveskip=1cm,label=paraanal,caption=N-grammianalyysin
  ensimmäinen vaihe.]
parameter_analyzer *.pf.gz +RTS -N
\end{lstlisting} 

\subsection{Matriisien muodostaminen tietorakenteista}

Tämän vaiheen yhteydessä aineistossa esiintyvät eri tyyppiset arvot
numeroidaan. Nämä arvot sijoitetaan matriisin sarakkeisiin taulukon
\ref{matriisi} mukaisesti.
Näistä kolme ensimmäistä saraketta yksilöivät HTTP-kyselyn. 
Tietojen avulla on mahdollista hakea matriisin riviä vastaava
palvelinlokin rivi. Ominaisuutta hyödynnetään analyysivaiheessa, kun
poikkeaviksi havaittuja pisteitä tutkitaan. 

Sarakkeissa 4 ja 5 on HTTP-kyselyn saapumisajankohdasta laskettu tunti ja viikonpäivä. 
Tämä ajankohta määräytyy palvelimen kellon mukaan ja aikavyöhykeenä on UTC. Lokista ei kuitenkaan
nähdä miltä aikavyökkeeltä kysely on peräisin.

Kyselyssä esiintyvä HTTP-metodi ja HTTP-protokollan versio ovat
esitetty sarakkeissa 6 ja 7. Metodit on numeroitu taulukon
\ref{metodi} mukaisesti, ja protokollan versio puolestaan taulukon
\ref{httpversio} mukaisesti. Numeerinen HTTP-vastauskoodi esiintyy
sellaisenaan sarakkeessa 8.

\begin{table}[p]
\centering
\begin{tabular}{l|l}
sarake & sisältö \\
\hline

1 & tiedoston järjestysnumero \\
2 & palvelimen tunnistenumero \\
3 & rivinumero lokitiedostossa \\
4 & tunti (0-23) \\
5 & viikonpäivä (maanantai = 1, ... , sunnuntai = 7) \\
6 & HTTP-metodi \\
7 & HTTP-protokollan versio \\
8 & HTTP-vastauskoodi \\
9 & siirretyn datamäärän suuruusluokka \\
10 & resurssin numero \\
11 ... & lista n-grammien esiintymistiheyksistä \\

\end{tabular}
\caption{Matriisin sarakkeiden sisältö.}
\label{matriisi}
\end{table}

\begin{table}[p]
\centering
\begin{tabular}{ll}
metodi & ryhmä \\
\hline
HEAD & 1 \\
GET & 2 \\
POST & 3 \\
PUT & 4 \\
DELETE & 5 \\
TRACE & 6 \\
OPTIONS & 7 \\
CONNECT & 8 \\
PATCH & 9 \\

\end{tabular}
\caption{HTTP method -kentän numerointi.}
\label{metodi}
\end{table}

\begin{table}[p]
\centering
\begin{tabular}{llll}
versio & ryhmä \\
\hline
HTTP/1.0 & 1 \\
HTTP/1.1 & 2 \\
\end{tabular}
\caption{ HTTP:n versiokentän numerointi.}
\label{httpversio}
\end{table}

Kyselyn seurauksena siirretyn datamäärän suuruusluokka ilmaistaan
sarakkeessa 9. Suuruusluokka lasketaan funktion \ref{modlog} avulla, jossa x
on siirretyn datan määrä tavuina.

\begin{equation} \label{modlog}
  \chi(x) = \left\{
  \begin{array}{ll}
    0 & \quad \text{, kun $x = 0$}\\
    \lfloor log_2 \, x \rfloor & \quad \text{, kun x > 0}\\
  \end{array} \right.
\end{equation}

\vskip 0.5cm
Sarakkeessa 10 ilmaistaan resurssin numero. Koska jokaisesta
resurssista muodostetaan erillinen matriisi, tämän sarakeen arvo on
vakio jokaisessa matriisissa.

% Merkkijonoja sisältävät arvot toimivat kukin omana ryhmänään.

Käyttämässämme datassa olevat kyselyiden parametrit sisältävät hyvin
samantyyppisiä arvoja, joten niistä muodostuu kohtuullisen
pieniulotteisia 2-\-grammitaulukoita. Tässä tutkimuksessa käytetyllä
aineistolla suurin esiintynyt 2-grammien lukumäärä oli
189. Käytännössä havaittiin, että käyttämällämme laitteistolla alle
200 saraketta sisältävän matriisin laskenta oli riittävän
nopeaa. Tämän vuoksi dimensioita ei tarvitse vähentää, joten matriiseihin ei sovelleta
satunnaisprojektiota. Sen sijaan muodostetaan matriisi, jossa
luetellaan kussakin HTTP-kyselyssä esiintyvien N-grammien
esiintymistiheydet. Näin muodostetun matriisin tiedot liitetään
aiemmin kerättyjen parametrien perään.

Analyysissä käytettävät matriisit muodostetaan
\texttt{parameter2vector}-\-sovelluksella. Listauksessa \ref{p2v}
esiintyvä parametri \texttt{ngram} korvataan edellisessä vaiheessa
muodostuneella N-\-grammitiedoston nimellä, \texttt{lahde} korvataan
tietorakennetiedoston nimellä ja parametrissa \texttt{kohde}
määritellään muodostettavien matriisitiedostojen nimen alkuosa.

\begin{lstlisting}[float=h,language=bashshell,label=p2v,aboveskip=0.5cm,caption=Matriisien
  muodostaminen.]
parameter2vector ngram lahde kohde
\end{lstlisting} 

Ajon seurauksena muodostuu jokaisesta resurssista erillinen
matriisi. Kukin matriisi tallennetaan resurssin mukaan nimettyyn
tiedostoon CSV-muodossa. Siinä matriisin rivit erotetaan toisistaan
rivinvaihdolla, ja sarakkeet pilkulla. Listauksessa \ref{csv882} on
poiminta CSV-muotoon tallennetusta matriisista.

\begin{lstlisting}[float=h,label=csv882,aboveskip=0.5cm,caption=Esikäsiteltyä
  dataa CSV-muodossa esitettynä.]
6,3,13619,10,2,2,2,200,12,882,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
7,3,833,21,5,2,2,200,12,882,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8,2,2324,23,3,2,2,200,12,882,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
8,2,18596,15,4,2,2,200,12,882,1,1,1,1,1,2,1,1,1,1,1,0,0,0,0,0
8,2,14500,12,4,2,1,200,12,882,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
\end{lstlisting}

\pagebreak

\section{Diffuusiokuvausten laskenta}
\label{sec:matlab}

Tässä tutkimuksessa käytettiin diffuusiokuvaussovellusta, joka on
toteutettu Jyväskylän yliopistossa Tuomo Sipolan toimesta. Sovellus
toimii Matlab-ympäristössä ja se lukee taulukon \ref{matriisi} mukaisesti
muotoiltuja tiedostoja ja palauttaa analyysin tulokset CSV-muodossa,
jonka rakenne on esitelty taulukossa \ref{zipolafile}.


\begin{table}[h]
\centering
\begin{tabular}{l|l}
sarake & sisältö \\
\hline
1 & tiedoston järjestysnumero \\
2 & palvelimen tunnistenumero \\
3 & rivinumero lokitiedostossa \\
4 & 2. ominaisvektorin arvo \\
5 & 3. ominaisvektorin arvo \\
6 & 4. ominaisvektorin arvo \\
7 & poikkeavuusluku \\

\end{tabular}
\caption{Tulosmatriisin sarakkeiden sisältö.}
\label{zipolafile}
\end{table}

\begin{lstlisting}[float=h,label=coords882,aboveskip=0.5cm,caption=Analysoitua
 dataa CSV-muodossa esitettynä.]
6,3,13619,-66.236,-0.23311,2.3295,0.0001666
7,3,833,-66.236,-0.23311,2.5856,0.0012385
8,2,2324,-66.236,-0.23311,2.3297,0.0001666
8,2,18596,-284.17,-6.6455e-06,1.3067e-12,653.84
8,2,14500,-66.236,-0.23311,-83.326,1.5157
\end{lstlisting}

Diffuusiokuvausalgoritmin aikavaativuus on eksponentiaalinen suhteessa
opetusmateriaalin kokoon. Tämän vuoksi diffuusiokuvausten laskentaan
voidaan käytännössä käyttää korkeintaan 5~000 yksittäistä HTTP-kyselyä
vastaavaa pistettä.  Ajan säästämiseksi tässä tutkimuksessa käytettiin
kuitenkin enintään 2~000 pisteen kokoisia aineistoja. Pisteiden määrän
rajoittamiseen käytettiin satunnaisotannan suorittavaa apuohjelmaa,
joka sijaitsee PhasefulSplitter-sovelluksen tiedostossa \\
\texttt{src/LinePicker.hs}.

\section{HTTP-kyselyiden yhdistäminen tuloksiin}

Esikäsittelyvaiheessa jokaiseen HTTP-kyselyyn liitetään
LineInfo-tietorakenne, jota kuljetetaan analyysivaiheen
läpi. Analyysin tuloksena saadaan taulukon \ref{zipolafile} mukaisia
CSV-muodossa olevia matriiseja. Tietorakenne tallennetaan matriisin
kolmeen ensimmäiseen sarakkeeseen, eli tekstimuotoisessa esityksessä
se sijaitsee rivin kolmessa ensimmäisessä pilkulla erotetussa
kentässä. Esimerkiksi listauksessa \ref{csv882} ensimmäisen pisteen
LineInfo-arvot ovat 6, 3 ja 13619.

Jotta analyysin tulokset olisivat hyödynnettävissä, on tarpeellista
yhdistää tulokset alkuperäisiin HTTP-kyselyihin. Tällöin voidaan
tarkemmin tutkia, millaisia poikkeavat kyselyt ovat
muodostaan. Yhdistämisessä käytettävä apuohjelma sijaitsee tiedostossa
\texttt{src/LineInfoConvert.hs}. Sen käyttö on esitelty
listauksessa \ref{backtrack}.
Parametriin \texttt{tulosmatriisi} sijoitetaan analyysin tuloksena
saadun matriisin tiedostonnimi. Parametri \texttt{lista} korvataan
palvelun tiedostolistalla, joka on muodostettu luvussa
\ref{sec:tiedostolista} esitetyllä tavalla. Yhdistetyt tulokset
kirjoitetaan parametrissa \texttt{kohde} määriteltyyn tiedostoon,
jonka rakenne on kuvattu taulukossa \ref{yhdistetty}.

\begin{lstlisting}[language=MyHaskell,float=h,caption=HTTP-kyselyiden 
  yhdistäminen tuloksiin.,label=backtrack,aboveskip=1cm]
a <- readZipolaFile tulosmatriisi
saveLogLines lista kohde a
\end{lstlisting}

\begin{table}[h]
\centering
\begin{tabular}{l|l}
sarake & sisältö \\
\hline
1 & tiedoston järjestysnumero \\
2 & palvelimen tunnistenumero \\
3 & rivinumero lokitiedostossa \\
4 & HTTP-kysely Combined Log Format -muodossa \\

\end{tabular}
\caption{Yhdistetty tulosmatriisi.}
\label{yhdistetty}
\end{table}


%anomaliat resursseittain ryhmiteltynä:

%backtrackCSVnumeric
%tiedostolistaus
%vektorihakemisto
%kohde
%[1,18,337,721,723,882]
