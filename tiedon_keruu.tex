% -*- mode: LaTeX; coding: utf-8; -*-

\chapter{Tiedon keruu ja esikäsittely}

TODO.

 % - Mistä data on peräisin?
 %    - Palvelun rakenteen kuvaus
 % - Millaista data on?
 %   - HTTP
 %   - Apachen logit
 % - Miten data on kerätty? (ohjelmistot ja verkkotopologia)
 %   - Yleisellä tasolla
 % - Mitä työkaluja on käytetty (Haskell ja PhasefulSplitter)
 % - Miten dataa on käsiteltu ja suodatettu?

\section{Tiedon rakenne}

Luvussa \ref{sec:lahtokohta} kuvailtiin tutkittavien lokitiedostojen
alkuperää ja tiedostomuotoa.

Web-hotelli on toteutettu siten, että yksittäiset palvelut on
sijoitettu jokaiselle palvelimelle. Ratkaisun taustalla on
kuormituksen tasaaminen. Erillinen järjestelmä huolehtii siitä, että
Internetistä tulevat kyselyt ohjataan tasaisesti eri
palvelimille. Taulukossa \ref{nimet} on esimerkki palveluiden ja
palvelinten nimeämisestä.

% Ei kannata ottaa esimerkkiä tästä taulukosta, tämä on vähän mutkikas.
\begin{table}[h]
\centering
\begin{tabular}{lll}
Palvelin && Palvelu \\
\cline{1-1}\cline{3-3}
dapper && buzz \\
edgy && rex \\
feisty && potato \\
&& hamm \\
&& slink \\
\end{tabular}
\caption{Palvelinten ja palveluiden nimeäminen.}
\label{nimet}
\end{table}

Lokitiedostot on sijoitettu hakemistorakenteeseen, jossa juuressa ovat
palvelinten nimien mukaiset hakemistot, joiden sisällä sijaitsevat
lokitiedostot, jotka on nimetään yhdistämällä palvelun nimi
päivämääräleimaan. Tiedostot on pakattu
\texttt{gzip}-pakkausohjelmalla. Taulukossa \ref{tiedostot}
havainnollistetaan tiedostonnimien muodostumista.

\begin{table}[h]
\centering
\begin{tabular}{llll}
Palvelin & Palvelu & Päivämäärä & Tiedostonnimi \\
\hline
edgy & buzz & 14.6.2009 & \texttt{edgy/buzz.http.2009-06-14.gz}\\ 
dapper & potato & 2.7.2009 & \texttt{dapper/potato.http.2009-07-02.gz}\\
feisty & rex & 30.7.2009 & \texttt{feisty/rex.http.2009-07-30.gz}\\
\end{tabular}
\caption{Tiedostojen nimeäminen.}
\label{tiedostot}
\end{table}


\section{Esikäsittely}

\section{Analysoinnin vaiheet}

Analysointi koostuu kolmesta vaiheesta.

Toisessa vaiheessa tekstimuotoiset lokitiedostot luetaan ja
käsitellään koneellisesti helpommin analysoitavaan
muotoon. Tätä työtä varten kehitetyssä tiedonkäsittelijässä
lokitiedoston eri kentät palastellaan ja tieto tallennetaan
relaatiotietokantaan. Lopuksi eri kenttien numeeriset ja
luokka-asteikolliset arvot klusteroidaan.

Kolmannessa vaiheessa tapahtuu varsinainen analysointi. Käytetty
anomalia-analyysi edellyttää, että aineiston muuttujat ovat
luokka-asteikollisia ja tästä johtuen tieto on klusteroitu edeltävässä vaiheessa.

\subsection{Tiedon keruu}

TODO.

\subsection{Esikäsittely}

Sopivaa yksivaiheista parseria käyttämällä olisi mahdollista käsitellä
lähtödata suoraan analyysissä käytettävään muotoon. Käytännössä kuitenkin datan
esikäsittely kannattaa hoitaa useammassa vaiheessa, jotta datassa
olevat puuttuvat tai poikkeavat arvot tulee huomioitua
asianmukaisesti. Monivaiheinen esikäsittely helpottaa myös saman
lähtödatan käyttämisen useaan eri analyysiin.

Tiedon käsittelyn helpottamiseksi tässä työssä käytetään esikäsittelyn
välivaiheiden ja lopputuloksen tallentamiseen
relaatiotietokantaa. Relaatiotietokanta mahdollistaa useiden
esikäsittelyn vaiheiden suorittamisen vähäisellä ohjelmoinnilla ja
helposti ymmärrettävästi.

Tässä työssä käsiteltävää aineistoa varten on kehitetty ``PhasefulSplitter
'' -sovellus esikäsittelyä varten.

TODO.

\subsection{Kategorisointi}

TODO.
