% -*- mode: LaTeX; coding: utf-8; -*-

\chapter{Tutkimuksessa käytetyt menetelmät}

Edellisissä luvuissa on esitetty osa niistä hyökkäyksistä, joita vastaan verkot ja Web-palvelut joutuvat nykyisin suojautumaan. Niiden suuresta määrästä johtuen on ilmiselvää, että täysin 
turvallista ympäristöä on mahdoton rakentaa. Tätä ei edes pidetä tietoturvasuunnittelun lähtökohtana, vaan tärkeämpää on löytää tasapaino palvelun saatavuuden ja käytettyjen tietoturvaratkaisuiden 
välillä. Liian raskaat menetelmät aiheuttavat ylimääräistä viivettä palvelun tai verkon saatavuuteen, ja liian kevyet ratkaisut jättävät ne avoimiksi tietoturvahyökkäyksille. 

Ongelmia lokin analysoimisessa aiheuttaa aineiston suuri määrä, ja siinä esiintyvien parametrien tyypit. Parametreista osa on luokka-asteikollisia ja osa puolestaan numeroasteikollisia, joten tietynlaisten toimintamallien 
etsiminen näistä suoraan on haastavaa. Parametrien suuri määrä aiheuttaa myös laskennallisia ongelmia, joten niiden määrää tulee pystyä jotenkin vähentämään säilyttäen kuitenkin mahdollisimman tarkkaan alkuperäisten
muuttujien piirteet. Seuraavaksi esitellään yleisellä tasolla analysoinnissa käytetyt tekniikat, ja tätä seuraa näiden tarkempi matemaattinen kuvaus.


\section{Menetelmien yleinen kuvaus}
 
Anomalioiden tunnistamiseen käyttämämme järjestelmä pohjautuu diffuusiokuvausten ja diffuusioetäisyyksien käyttöön. Ne tarjoavat tehokkaan tavan löytää merkittäviä geometrisia rakenteita datasta, ja niiden
käyttämistä moniulotteisen datan esittämisessä on esitelty \cite{diff} \cite{diff2}. Menetelmien tehokkuus perustuu siihen, että diffuusiokuvausten avulla pystytään vähentämään analysoitavan datan dimensioita
säilyttäen kuitenkin sen rakenne. Periaatteessa dimensioiden vähentäminen tarkoittaa sitä, että datajoukko esitetään toisella datajoukolla, jonka dimensio on pienempi. Tällöin sen klusteroiminen sekä 
analysoiminen ja esittäminen graafisesti on helpompaa.

Datan luonteesta johtuen pelkkä dimensioiden vähentäminen ei tuo esille poikkeavuuksia, vaan tätä varten tarvitaan parametreja, jotka kuvaavat HTTP-pyynnön sisältöä tarkemmin. Suurimmasta osasta kyselyn
sisältämistä parametreista kuten IP-osoitteesta, ajasta tai käytetystä selaimesta tämä ei käy ilmi. Toki näistä voidaan tunnistaa esimerkiksi hyökkäykset, joissa yritetään kuluttaa palvelimen resurssit loppuun
hakemalla samaa resurssia yhä uudestaan tai pommittamalla uusia yhteysyrityksiä. Web-palveluihin kohdistuvat hyökkäykset ovat kuitenkin usein paljon hienovaraisempia. Parhaiten näitä voidaan yrittää tunnistaa
tutkimalla tarkemmin GET-parametrin jälkeistä osaa, josta käy ilmi parametrit, joita hyökkääjä välittää palvelimelle. 

Dimensioiden vähentäminen tapahtuu laskemalla diffuusioetäisyydet eli keskimääräiset arvot kaikista kahden pisteen välisistä poluista ts. todennäköisyydet kulkea satunnaiskululla pisteestä toiseen kiinteällä
askelmäärällä. Ennen tätä analysoitava data tulee muuttaa kategoriseksi, sillä muuten erilaisten parametrityyppien välisiä etäisyyksiä ei pystyä laskemaan. Osa parametreista on jo valmiiksi kategorisessa 
muodossa, mutta numeerinen data tulee erikseen kategorisoida. Numeerisen datan automaattinen kategorisointi tapahtuu klusteroimalla yhtä ominaisuutta, ja laskemalla klusteroinnin hyvyysarvo. Tätä jatketaan niin pitkään,
kunnes optimaalinen klusterointi on saavutettu. Prosessi toistetaan vaihtaen kategorioiden lukumäärää jokaisessa iteraatiossa, ja se luku, joka tuottaa parhaimman arvon, valitaan optimaaliseksi kategorioiden 
määräksi .

Tietoturvahyökkäyksissä hyökkääjä pyrkii aina ohittamaan jollakin tavalla asetetut suojaukset. Usein tämä tarkoittaa sitä, että palvelimelle välitetyt pyynnöt muodostuvat pitkistä merkkijonoista, ja niissä käytetyt
merkit poikkeavat tyypillisesti käytetyistä merkeistä. Useiden peräkkäisten avain-arvo parien määrä myös saattaa kasvaa reilusti tavallista suuremmaksi. Näiden tunnistamista varten käytämme analyysissa n-gram -analyysiksi
kutsuttua menetelmää. Menetelmällä lasketaan datassa esiintyvien peräkkäisten merkkien tai sanasten esiintyvyystiheyksiä. Analyysi voidaan tehdä esimerkiksi koko kyselylle, avain-arvo -pareille tai avainten nimille. 

Suurilla tietomassoilla n-gram -analyysi tuottaa isoja matriiseja, joiden käsitteleminen on hidasta. Tehokkuuden takia matriisien ulottuvuuksia tulee pystyä jollakin tavalla vähentämään. Tämä onnistuu satunnaisprojektion 
avulla, joka on ulottuvuuksien vähentämiseen tarkoitettu menetelmä. Satunnaisprojektiossa moniulotteinen data heijastetaan pienempiulotteiseen aliavaruuteen käyttäen satunnaisesti luotua matriisia. Näin syntynyt uusi 
matriisi on laskennallisesti tehokas, ja se säilyttää tässä tapauksessa riittävän määrän informaatiota. 

\section{Diffuusiokuvaus}

Moniulotteisen datan analysoiminen on aina haasteellinen tehtävä johtuen parametrien suuresta määrästä. Tämän takia käytämme tässä työssä hyödyksi diffuusiokuvauksia, jotka ovat tehokas tapa vähentää analysoitavan datan
ulottuvuuksia säilyttäen kuitenkin sen rakenne. Tämän jälkeen esimerkiksi klusterointi ja visualisointi pienempiulotteisessa avaruudessa on helpompaa.

Ensimmäiseksi diffuusiokuvauksiin perustuvalle järjestelmälle opetetaan datan normaali käyttäytyminen. Tämä tapahtuu käyttäen opetusmateriaalia, joka on osa analysoitavaa dataa. Olkoot tämä opetusmateriaali 
$\Gamma = \left\{ x_1, x_2, \dots , x_N \right\}, x_i \in \mathbb{R}^n$, jossa $N$ on kyselyiden määrä ja $n$ alkuperäisen datan ulottuvuuksien määrä. Meidän tapauksessamme data muodostaa $N \times n$ matriisin, jossa 
yksittäinen rivi kuvaa yhtä HTTP-kyselyä ja sarakkeet vastaavat HTTP-kyselyistä määritettyjä parametreja.

Aluksi luodaan samankaltaisuusmatriisi $W$, joka kuvaa pisteiden välisiä etäisyyksiä.
Matriisi muodostetaan käyttäen painotettua Hammingin etäisyyttä. Tämä tapahtuu valitsemalla tutkittavasta matriisista $\Gamma$ rivit $x_i$ ja $x_j$ sekä merkitsemällä yhden sarakkeen kategorioiden määrää $\pi_k$:lla.

%\begin{equation*}
%W_{ij} = e^{-\frac{\delta_{WH}(x_i, x_j)}{\epsilon}}
%\label{KERNEL}
%\end{equation*}

\begin{equation*}
W_{ij} = \exp\left(-\frac{\delta_{WH}(x_i, x_j)}{\epsilon}\right)
\label{KERNEL}
\end{equation*}

\noindent Datapisteiden välinen painotettu Hammingin etäisyys lasketaan summaamalla sarakkeiden väliset Hammingin etäisyydet ja jakamalla se kussakin sarakkeessa esiintyvien kategorioiden määrällä.

\begin{equation*}
\delta_{WH}(x_i,x_j) = \sum_{k=1}^{n} \frac{\delta(x_{ik},x_{jk})}{\pi_k}
\label{W_HAMMING}
\end{equation*}

\noindent Kahden alkion välinen Hammingin etäisyys määritellään seuraavasti:

\begin{equation*}
\delta(x_{ik},x_{jk}) = 
\begin{cases}
0 & \text{, kun } x_{ik} = x_{jk} \\
1 & \text{, muutoin}
\end{cases}
\label{HAMMING}
\end{equation*}

\noindent Tässä käytetään gaussisen ytimen etäisyysmittana $\delta_{WH}$:aa. Pisteiden samankaltaisuuden toisiinsa nähden määrittelee $\epsilon$. 

% TODO mikä on W? Se kernelimatriisiko?

Parametrin $\epsilon$ tulee olla riittävän suuri kattaakseen
riittävästi ympäröiviä pisteitä, mutta ei kuitenkaan niin suuri, että
se hävittäisi pisteiden väliset etäisyydet. Tässä tutkimuksessa
käytetään laskennallisesti tehokasta Lafonin menetelmää, joka on
kuvattu B. Bahin tutkielmassa\cite{bah}. %s. 18
Menetelmässä lasketaan keskiarvo funktion $\delta_{WH}(x_i,x_j)$ minimiarvoista, kun $x_i \ne x_j$. 

\begin{equation*}
\epsilon = \frac{1}{k} \sum_{i=1}^k \min_{j:x_j \ne x_i} \delta_{WH}(x_i, x_j)
\label{EPSILON}
\end{equation*}

\noindent Näin luodun matriisin rivit normalisoidaan diagonaalimatriisin $D$ avulla, joka luodaan yhtälössä \ref{ROWSUM}. 

\begin{equation}
D_{ii} = \sum_{j=1}^{N} W_{ij}
\label{ROWSUM}
\end{equation}

\noindent Nyt jokaisen rivin summaksi tulee 1. $P$ kuvaa todennäköisyyttä sille, että yksittäinen alkio muuttuu tilasta toiseen.

\begin{equation}
P = D^{-1} W
\label{PROB}
\end{equation}

\noindent Seuraavaksi määritellään muuttumistodennäköisyysmatriisin ominaisarvot. $P$:n ominaisarvot ovat samat kuin konjugaattimatriisin, joka on esitetty yhtälössä \ref{SYMM}. Tämän matriisin ominaisarvot voidaan johtaa matriisista $\tilde{P}$, kuten myöhemmin tulee ilmi.

\begin{equation}
\tilde{P} = D^{\frac{1}{2}} P D^{-\frac{1}{2}}
\label{SYMM}
\end{equation}

\noindent Jos vaihdamme $P$:n yhtälöstä \ref{SYMM} yhtälössä \ref{PROB} olevan kanssa, saamme yhtälön \ref{NGL} symmetrisen todennäköisyysmatriisin $\tilde{P}$. Näin luotu matriisi säilyttää ominaisarvonsa, ja sitä kutsutaan normalisoidun graafin Laplace-operaatioksi.

\begin{equation}
\tilde{P} = D^{\frac{1}{2}} P D^{-\frac{1}{2}} = D^{\frac{1}{2}} D^{-1} W D^{-\frac{1}{2}} = D^{-\frac{1}{2}} W D^{-\frac{1}{2}}
\label{NGL}
\end{equation}

\noindent Tämän jälkeen symmetrinen matriisi hajotetaan käyttäen singulaarihajotelmaa (engl. \textit{Singular Value Decomposition}, SVD). Koska $\tilde{P}$ on normaalimatriisi, spektraaliteorian (engl. \textit{spectral theorem}) mukaan tällaisen matriisin 
hajotelma on yhtälön \ref{SVD} mukainen.  

\begin{equation}
\tilde{P} = U \Lambda U^*
\label{SVD}
\end{equation}

% TODO mitä tarkoittaa diag? Miksi sillä operoidaan pikku-lambdoja?

\noindent Matriisissa $\Lambda$ diagonaalilla olevat singulaariarvot $diag([\lambda_1, \lambda_2, \dots, \lambda_N])$ vastaavat matriisissa $\tilde{P}$ olevia ominaisarvoja, koska se on symmetrinen. Edelleen koska $\tilde{P}$ on konjugaatti $P$:n kanssa, sisältävät nämä kaksi samat ominaisarvot. 

Matriisin $U = [ u_0, u_1, \dots, u_k ]$ sarakkeet sisältävät
matriisin $\tilde{P}$ $k$ ominaisvektoria $u_k$. Käyttämällä yhtälöä
\ref{EIGENVECTORS} voidaan laskea matriisin $P$ oikeat ominaisvektorit $v_k$, jolloin 
saamme ne matriisin $V$ sarakkeina  $V = [v_0, v_1, \dots, v_k]$.  

\begin{equation}
V = D^{-\frac{1}{2}} U
\label{EIGENVECTORS}
\end{equation}

\noindent Nyt datan koordinaatit pienennetyssä avaruudessa ovat yhtälön \ref{MAP_COORDINATES} matriisissa $\Psi$. Matriisin rivit vastaavat datapisteitä ja sarakkeet näiden uusia pienennetyn avaruuden koordinaatteja.

\begin{equation}
\Psi = V \Lambda
\label{MAP_COORDINATES}
\end{equation}

% TODO tarviiko mainita, että t=1 ?
% TODO decay of spectrum? lähestyvätkö nollaa vai mitä tahansa?

\noindent Käyttämällä sopivaa arvoa $\epsilon$ lähestyvät ominaisarvot nopeasti nollaa, jolloin riittävän tarkkaan diffuusiokuvaukseen tarvitaan ainoastaan $d$ komponenttia. Ensimmäinen ominaisvektori $v_0$ on vakio, joten se voidaan jättää pois.
Käyttäen ainoastaan ensimmäiset $d$ komponenttia diffuusiokuvauksen luomiseen on esitetty yhtälössä \ref{DM}.

\begin{equation}
\Psi_d : x_i \to \left[ \lambda_1 V_{i1}, \lambda_2 V_{i2}, \dots, \lambda_d V_{id} \right]
\label{DM}
\end{equation}

\noindent Tämä diffuusiokuvaus upottaa tunnetut pisteet $x_i$ $d$-ulotteiseen avaruuteen. 
Näin $n$-ulotteisesta datasta on muodostettu $d$-ulotteinen diffuusioavaruus.

Tarvittaessa diffuusiokuvaus voidaan skaalata jakamalla sen koordinaatit luvulla $\lambda_1$.

\section{N-grammianalyysi}

% TODO lähde n-grammeista

Tässä tutkielmassa on tarpeen analysoida tarkemmin HTTP:n GET-parametrin jälkeistä kyselyosuutta. Tarkoitusta varten tarvitaan menetelmä, joka toimii riittävän nopeasti ja tehokkaasti, mutta tuottaa kuitenkin hyödyllistä tietoa parametrien ominaisuuksista. N-\-grammianalyysi on hyvin tunnettu ja käytetty menetelmä, jolla tutkitaan 
peräkkäisten merkkien tai sanasten esiintymistiheyttä. Sitä käytetään laajalti muun muassa tilastollisen kielen analyysissa, jossa esimerkiksi puheentunnistuksessa sillä tutkitaan foneemeja eli kielen äänneyksikköjä. 

Tässä tutkimuksessa yksiköt ovat merkkejä, joiden esiintymistiheyttä ja jakaantumista analysoidaan. Merkkien $n$-grammit muodostetaan asettamalla $n$:n merkin pituinen ikkuna merkkijonon alkuun, jonka jälkeen ikkuna liu'utetaan merkki kerrallaan merkkijonon yli, keräten näin muodostuneet osamerkkijonot. Esimerkiksi merkkijonon \texttt{automaatti} 2-grammit ovat
\texttt{au},
\texttt{ut},
\texttt{to},
\texttt{om},
\texttt{ma},
\texttt{aa},
\texttt{at},
\texttt{tt} ja
\texttt{ti}.
Näin muodostetusta n-grammien listasta voidaan muodostaa n-grammianalyysin avulla vektori, jonka alkioihin asetetaan kunkin n-grammin esiintymistiheys. Näin muodostettua vektoria voidaan käyttää hyödyksi merkkijonon samankaltaisuuden arvioinnissa.

N-grammianalyysin tuottamien vektoreiden pituus on $m^n$, jossa $m$ on datassa esiintyvien sanasten määrä ja $n$ on n-grammisarjan pituus. Tutkimuksessa analysoidaan vain kahden peräkkäisen merkin esiintyvyyksiä, 
jolloin $n=2$, ja koska tutkittavat sanaset ovat 8-bittisiä merkkejä, niin $m=2^8=256$. Informaation määrää pystytään vähentämään jonkin verran jättämällä huomiotta ne vektorin alkiot, jotka sisältävät nollan jokaisessa tutkittavassa vektorissa. Tästäkin huolimatta alkioita on niin runsaasti, että niistä muodostetun avaruuden ulottuvuuksien määrä tulee pystyä jollakin tavoin vähentämään. 

\section{Satunnaisprojektio}

Satunnaisprojektiossa alkuperäinen $N$-ulotteinen data heijastetaan $k$-ulotteiseen $(k \ll N)$ aliavaruuteen käyttäen satunnaista $k \times N$ matriisia $R$ \cite{Random}. Olkoot meillä esimerkiksi matriisi 
$X_{m\times N}$, jossa $m$ on havaintojen määrä, ja $N$ on datan alkuperäinen dimensio. Olkoot  $k$  sitten uusi haluttu dimensioiden määrä. Uuden matriisin laskemiseksi luodaan satunnainen matriisi 
$R_{n \times k}$, jossa jokaisen sarakkeen arvot ovat satunnaisesti jakautuneet. Kertomalla nämä keskenään saadaan matriisi $X_{m \times k}^{RP}$, joka on esitys alkuperäisestä datasta $X$ heijastettuna $k$-ulotteiseen 
aliavaruuteen:

\begin{equation}
X_{m \times k}^{RP} = X_{m \times N} \cdot R_{n \times k}.
\label{RP}
\end{equation}

Satunnaisprojektion idea on lähtöisin Johnson-Lindenstraussin lemmasta: jos vektoriavaruudessa olevat pisteet heijastetaan satunnaisesti valittuun aliavaruuteen jossa on sopiva määrä ulottuvuuksia, säilyvät pisteiden
väliset etäisyydet riittävällä tarkkuudella. $X_{m \times N} \cdot R_{n \times k}.$ laskemisen aikavaativuus on $O(dkN)$, ja jos matriisi $X$ sisältää pääasiallisesti nollia ja rivissä on keskimääräisesti $c$ kappaletta arvoja 
$(c \ll N)$, on aikavaativuus $O(ckN)$.

Satunnaisesti luotu matriisi $R$ voidaan valita monella eri tapaa. Useimmiten matriisin $R$ elementit $r_{ij}$ noudattavat Gaussin jakaumaa, mutta se voidaan muodostaa myös muulla tavoin kuten esimerkiksi

\begin{equation}
r_{ij} = \sqrt{3}\cdot 
\begin{cases}
 +1 &\text{todennäköisyydellä $\frac{1}{6}$} \\
 0 &\text{todennäköisyydellä $\frac{2}{3}$} \\
 -1 &\text{todennäköisyydellä $\frac{1}{6}$} \\
\end{cases}
\label{RPChoice}
\end{equation}

Tällaisen jakauman käyttäminen vähentää entisestään laskenta-aikaa, sillä laskenta voidaan suorittaa käyttäen kokonaislukuja. Yllä olevan jakauman tapauksessa laskenta on vieläkin nopeampaa, sillä operaatioista
tarvitaan vain kolmasosa, sillä luotu matriisi sisältää suurimmaksi osaksi nollia \cite{Random}.
